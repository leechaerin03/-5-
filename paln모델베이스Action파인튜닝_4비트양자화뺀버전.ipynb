{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leechaerin03/-5-/blob/main/paln%EB%AA%A8%EB%8D%B8%EB%B2%A0%EC%9D%B4%EC%8A%A4Action%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D_4%EB%B9%84%ED%8A%B8%EC%96%91%EC%9E%90%ED%99%94%EB%BA%80%EB%B2%84%EC%A0%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zBl9BsSuhc-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wandb"
      ],
      "metadata": {
        "id": "dHA8bmWriZmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# LLM 모니터링\n",
        "wandb.login(key=\"9ed675d942c745bf3fdf524bba1fcd137e6e81ab\")\n",
        "run = wandb.init(project='Fine tuning  LAM2', job_type='training', anonymous='allow')"
      ],
      "metadata": {
        "id": "R16JHGGki3DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gagDJf1PwAMi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    LoraConfig\n",
        ")\n",
        "\n",
        "\n",
        "# --- 1. 모델 및 토크나이저 ID ---\n",
        "hub_repo_name = \"leeChaerin/PlanModel_v1\"\n",
        "\n",
        "\n",
        "# --- 2. 모델 로드 ---\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    hub_repo_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# --- 3. 토크나이저 로드 ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(hub_repo_name, padding_side=\"left\")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"Tokenizer pad_token이 eos_token으로 설정되었습니다.\")\n",
        "\n",
        "# --- 4.LoRA 어댑터 적용 ---\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], # PlanModel_v1의 기반 모델(Kanana) 기준\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# 모델에 LoRA 어댑터 적용\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# --- 5. 준비 완료 ---\n",
        "print(\"\\n--- LoRA 어댑터 적용 후 학습 가능한 파라미터 ---\")\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIi-EnI8xxlT"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(tokenizer.eos_token)\n",
        "#base모델은 eos 토큰이 128001임 왜 차이나는진 모르겠음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WmRs0_YyBGN"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"\"\"You are a specialized AI agent for Dongguk Univ nDRIMS. Your task is to analyze the user's input structure and perform one of two specific tasks:\n",
        "    1.  **Plan Generation:**\n",
        "      If the user 'prompt' contains **only** a `user_request` (e.g., \"불교동아리 가입\"), this is a request for a plan.\n",
        "      You MUST respond with the complete `{\"task_plan\": [{\"step_id\": , \"description\":}...]}` JSON for the entire task.\n",
        "      If the user requests a specific course, include the specific course name in the plan description.\n",
        "\n",
        "    2.  **Action Generation:**\n",
        "      If the user 'prompt' contains a `task_plan` (current step) AND `observations` (current state), this is a request for an action.\n",
        "      You MUST respond with the single `{\"action\": {...}}` JSON for that specific step.\n",
        "      For input actions, ensure the `value` field matches the specific data from the plan.\"\"\"}\n",
        "]\n",
        "\n",
        "tokens = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "print(tokens)\n",
        "#print(tokenizer.encode(tokens, add_special_tokens=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_TEXT = (\n",
        "    \"\"\"You are a specialized AI agent for Dongguk Univ nDRIMS. Your task is to analyze the user's input structure and perform one of two specific tasks:\n",
        "    1.  **Plan Generation:**\n",
        "      If the user 'prompt' contains **only** a `user_request` (e.g., \"불교동아리 가입\"), this is a request for a plan.\n",
        "      You MUST respond with the complete `{\"task_plans\": [...]}` JSON for the entire task.\n",
        "\n",
        "    2.  **Action Generation:**\n",
        "      If the user 'prompt' contains a `task_plan` (current step) AND `observations` (current state), this is a request for an action.\n",
        "      You MUST respond with the single `{\"action\": {...}}` JSON for that specific step.\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "LbyCSwm8QF8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5XenGuZyMNh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "print(\"train_data.jsonl 업로드...\")\n",
        "uploaded = files.upload()\n",
        "DATA_PATH = list(uploaded.keys())[0]\n",
        "\n",
        "fixed_data = []\n",
        "print(\"데이터를 로드하며 타입을 문자열로 통일합니다...\")\n",
        "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            row = json.loads(line)\n",
        "\n",
        "            if isinstance(row.get('prompt'), dict):\n",
        "                row['prompt'] = json.dumps(row['prompt'], ensure_ascii=False)\n",
        "\n",
        "            if isinstance(row.get('output'), dict):\n",
        "                row['output'] = json.dumps(row['output'], ensure_ascii=False)\n",
        "\n",
        "            if 'prompt' in row and 'output' in row:\n",
        "                fixed_data.append(row)\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"JSON 파싱 오류 무시: {line}\")\n",
        "\n",
        "raw = Dataset.from_list(fixed_data)\n",
        "\n",
        "print(\"\\n--- 수정된 데이터로 로드 성공! ---\")\n",
        "print(raw)\n",
        "print(\"\\n[Row 0 예시] (Query -> Plan)\")\n",
        "print(raw[0])\n",
        "print(\"\\n[Row 1 예시] (Plan+State -> Action)\")\n",
        "print(raw[1])\n",
        "print(f\"\\n총 {len(raw)}개 데이터 로드 완료.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGl1PAKbyQcy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "qna_list = []\n",
        "\n",
        "SYSTEM_TEXT = (\n",
        "     \"\"\"You are a specialized AI agent for Dongguk Univ nDRIMS. Your task is to analyze the user's input structure and perform one of two specific tasks:\n",
        "    1.  **Plan Generation:**\n",
        "      If the user 'prompt' contains **only** a `user_request` (e.g., \"불교동아리 가입\"), this is a request for a plan.\n",
        "      You MUST respond with the complete `{\"task_plan\": [{\"step_id\": , \"description\":}...]}` JSON for the entire task.\n",
        "      If the user requests a specific course, include the specific course name in the plan description.\n",
        "\n",
        "    2.  **Action Generation:**\n",
        "      If the user 'prompt' contains a `task_plan` (current step) AND `observations` (current state), this is a request for an action.\n",
        "      You MUST respond with the single `{\"action\": {...}}` JSON for that specific step.\n",
        "      For input actions, ensure the `value` field matches the specific data from the plan.\"\"\"\n",
        ")\n",
        "\n",
        "for i in range(len(raw)):\n",
        "    state = raw[i][\"prompt\"]   # 사용자 입력(프롬프트)\n",
        "    action = raw[i][\"output\"]  # 정답\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_TEXT},\n",
        "        {\"role\": \"user\",   \"content\": state},\n",
        "    ]\n",
        "\n",
        "    prompt_ids = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=True, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    answer_ids = tokenizer.encode(\n",
        "        (action or \"\") + (tokenizer.eos_token or \"\"),\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "\n",
        "    input_ids = prompt_ids + answer_ids\n",
        "\n",
        "    prompt_str = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    answer_str = ((action or \"\") + (tokenizer.eos_token or \"\"))\n",
        "    input_str  = prompt_str + answer_str\n",
        "\n",
        "    qna_list.append({\n",
        "        \"prompt_ids\": prompt_ids,\n",
        "        \"answer_ids\": answer_ids,\n",
        "        \"input_ids\":  input_ids,\n",
        "        \"prompt_str\": prompt_str,\n",
        "        \"answer_str\": answer_str,\n",
        "        \"input_str\":  input_str,\n",
        "    })\n",
        "\n",
        "max_length = max(len(x[\"input_ids\"]) for x in qna_list)\n",
        "print(\"samples:\", len(qna_list))\n",
        "print(\"max_length:\", max_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1HAKB6GyUBe"
      },
      "outputs": [],
      "source": [
        "pad_id = tokenizer.pad_token_id\n",
        "assert pad_id is not None, \"pad_token이 설정돼 있어야 합니다.\"\n",
        "\n",
        "class SFTDataset(Dataset):\n",
        "    def __init__(self, items, max_length):\n",
        "        self.inputs = []\n",
        "        self.attns  = []\n",
        "        self.labels = []\n",
        "\n",
        "        for ex in items:\n",
        "            ids = ex[\"input_ids\"]\n",
        "\n",
        "            if len(ids) > max_length:\n",
        "                ids = ids[-max_length:]\n",
        "\n",
        "            attn = [1] * len(ids)\n",
        "            pad_len = max_length - len(ids)\n",
        "            if pad_len > 0:\n",
        "                ids  = ids  + [pad_id] * pad_len\n",
        "                attn = attn + [0]      * pad_len\n",
        "\n",
        "            prompt_len = len(ex[\"prompt_ids\"])\n",
        "            labels = [-100] * len(ids)\n",
        "            for pos in range(prompt_len, len(ex[\"input_ids\"])):\n",
        "                if pos >= max_length:\n",
        "                    break\n",
        "                labels[pos] = ids[pos]\n",
        "\n",
        "            self.inputs.append(torch.tensor(ids))\n",
        "            self.attns.append(torch.tensor(attn))\n",
        "            self.labels.append(torch.tensor(labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.attns[idx], self.labels[idx]\n",
        "\n",
        "dataset = SFTDataset(qna_list, max_length=min(1024, max_length))\n",
        "loader  = DataLoader(dataset, batch_size=2, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTbJQvKDyXAF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "torch.manual_seed(123)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVnOWDPvya2y"
      },
      "outputs": [],
      "source": [
        "# 파인튜닝 전에 어떻게 대답하는지 확인\n",
        "questions = [ qna['q_ids'] for qna in qna_list]\n",
        "\n",
        "for i, q_ids in enumerate(questions):\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            torch.tensor([q_ids]).to(\"cuda\"),\n",
        "            max_new_tokens=100,\n",
        "            #attention_mask = (input_ids != 0).long(),\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False,\n",
        "            # temperature=1.2,\n",
        "            # top_k=5\n",
        "        )\n",
        "\n",
        "    output_list = output.tolist()\n",
        "\n",
        "    print(f\"Q{i}: {tokenizer.decode(output[0], skip_special_tokens=True)}\")\n",
        "    print(\"----------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhkXO35k-AFZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZKK9p69zoBk"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "for epoch in range(10):\n",
        "    total = 0.0\n",
        "    for input_ids, attn_mask, labels in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attn_mask = attn_mask.to(device)\n",
        "        labels    = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
        "        loss = out.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "        wandb.log({\n",
        "            \"train/loss_step\": loss.item(),\n",
        "            \"epoch\": epoch,\n",
        "            \"step\": global_step,\n",
        "        })\n",
        "\n",
        "    avg_loss = total / len(loader)\n",
        "    losses.append(avg_loss)\n",
        "    print(f\"epoch {epoch} | loss {avg_loss:.4f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"train/loss_epoch\": avg_loss,\n",
        "        \"epoch\": epoch,\n",
        "    })\n",
        "\n",
        "    save_dir = f\"./adapters/epoch_{epoch:03d}\"\n",
        "    model.save_pretrained(save_dir)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(f\"[saved] LoRA adapter -> {save_dir}\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import IFrame, display\n",
        "\n",
        "\n",
        "display(IFrame(run.url, width=1200, height=720))\n"
      ],
      "metadata": {
        "id": "lc2pjKL2mDa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt1lBgeZLopX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_repo = \"leeChaerin/PlanModel_v1\"\n",
        "adapter_path = \"./adapters/epoch_009\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_repo)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_repo,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "model.eval()\n",
        "\n",
        "print(\"모델 로드 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxJ5ujNzPV7i"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(range(1, len(losses)+1), losses, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, json, torch\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "SYSTEM_TEXT = (\n",
        "     \"\"\"You are a specialized AI agent for Dongguk Univ nDRIMS. Your task is to analyze the user's input structure and perform one of two specific tasks:\n",
        "    1.  **Plan Generation:**\n",
        "      If the user 'prompt' contains **only** a `user_request` (e.g., \"불교동아리 가입\"), this is a request for a plan.\n",
        "      You MUST respond with the complete `{\"task_plan\": [{\"step_id\": , \"description\":}...]}` JSON for the entire task.\n",
        "      If the user requests a specific course, include the specific course name in the plan description.\n",
        "\n",
        "    2.  **Action Generation:**\n",
        "      If the user 'prompt' contains a `task_plan` (current step) AND `observations` (current state), this is a request for an action.\n",
        "      You MUST respond with the single `{\"action\": {...}}` JSON for that specific step.\n",
        "      For input actions, ensure the `value` field matches the specific data from the plan.\"\"\"\n",
        ")\n",
        "\n",
        "def build_inputs(user_prompt: str):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_TEXT},\n",
        "        {\"role\": \"user\",   \"content\": user_prompt},\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    return {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "def is_balanced_json_fragment(s: str) -> bool:\n",
        "    in_str = False\n",
        "    esc = False\n",
        "    stack = []\n",
        "    for ch in s:\n",
        "        if in_str:\n",
        "            if esc:\n",
        "                esc = False\n",
        "            elif ch == '\\\\':\n",
        "                esc = True\n",
        "            elif ch == '\"':\n",
        "                in_str = False\n",
        "            continue\n",
        "        else:\n",
        "            if ch == '\"':\n",
        "                in_str = True\n",
        "            elif ch == '{':\n",
        "                stack.append('}')\n",
        "            elif ch == '[':\n",
        "                stack.append(']')\n",
        "            elif ch in ('}', ']'):\n",
        "                if not stack or stack[-1] != ch:\n",
        "                    return False\n",
        "                stack.pop()\n",
        "    return len(stack) == 0 and len(s) > 0\n",
        "\n",
        "def longest_balanced_json(text: str) -> str | None:\n",
        "    # 첫 { 또는 [ 부터 스캔\n",
        "    start = None\n",
        "    for i, ch in enumerate(text):\n",
        "        if ch in '{[':\n",
        "            start = i\n",
        "            break\n",
        "    if start is None:\n",
        "        return None\n",
        "\n",
        "    in_str = False\n",
        "    esc = False\n",
        "    stack = []\n",
        "    last_good = None\n",
        "    for i, ch in enumerate(text[start:], start=start):\n",
        "        if in_str:\n",
        "            if esc:\n",
        "                esc = False\n",
        "            elif ch == '\\\\':\n",
        "                esc = True\n",
        "            elif ch == '\"':\n",
        "                in_str = False\n",
        "        else:\n",
        "            if ch == '\"':\n",
        "                in_str = True\n",
        "            elif ch == '{':\n",
        "                stack.append('}')\n",
        "            elif ch == '[':\n",
        "                stack.append(']')\n",
        "            elif ch in ('}', ']'):\n",
        "                if not stack or stack[-1] != ch:\n",
        "                    break\n",
        "                stack.pop()\n",
        "                if not stack:\n",
        "                    last_good = i + 1\n",
        "    return text[start:last_good].strip() if last_good else None\n",
        "\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "import re, torch\n",
        "\n",
        "class StopOnBalancedJSON(StoppingCriteria):\n",
        "    def __init__(self, tokenizer, lookback_tokens=400):\n",
        "        self.tok = tokenizer\n",
        "        self.lookback_tokens = lookback_tokens\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        txt = self.tok.decode(input_ids[0][-self.lookback_tokens:], skip_special_tokens=True)\n",
        "        # 마지막 JSON 시작 후보를 대략 잡아 균형 확인\n",
        "        m = re.search(r'([\\{\\[].*)$', txt, flags=re.DOTALL)\n",
        "        cand = m.group(1) if m else txt\n",
        "        return is_balanced_json_fragment(cand)\n",
        "\n",
        "stoppers = StoppingCriteriaList([StopOnBalancedJSON(tokenizer)])\n",
        "def generate_json(user_prompt: str, max_new_tokens=256, do_sample=False):\n",
        "    inputs = build_inputs(user_prompt)\n",
        "    input_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=do_sample,\n",
        "            temperature=0.7 if do_sample else None,\n",
        "            top_p=0.9 if do_sample else None,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            stopping_criteria=stoppers,\n",
        "        )\n",
        "\n",
        "    new_tokens = out_ids[0, input_len:]\n",
        "    text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "    # 균형 잡힌 JSON만 잘라서 파싱\n",
        "    jb = longest_balanced_json(text)\n",
        "    if jb is None:\n",
        "        return text\n",
        "    try:\n",
        "        return json.loads(jb)\n",
        "    except Exception:\n",
        "        return jb\n"
      ],
      "metadata": {
        "id": "CBCZBaPBLRms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 테스트 ----\n",
        "plan_prompt = \"{'user_request': '자퇴신청'}\"\n",
        "plan_result = generate_json(plan_prompt)\n",
        "print(plan_result)"
      ],
      "metadata": {
        "id": "9jISvRcOXfYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------모델 쓰는 방법 가이드----------\n",
        "\n",
        "# 1. 사용자 입력을 넣고, plan을 생성\n",
        "import json\n",
        "\n",
        "user_req = input(\"무엇을 하고 싶나요? (예: 휴학신청): \").strip()\n",
        "\n",
        "plan_payload = {\"user_request\": user_req}\n",
        "plan_prompt = json.dumps(plan_payload, ensure_ascii=False)\n",
        "\n",
        "plan_result = generate_json(plan_prompt, do_sample=False)\n",
        "print(\"[PLAN RESULT]\")\n",
        "print(plan_result)"
      ],
      "metadata": {
        "id": "U9zLvYrpdzCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, ast\n",
        "\n",
        "def extract_steps(plan_result):\n",
        "    if isinstance(plan_result, dict):\n",
        "        steps = plan_result.get(\"task_plan\") or plan_result.get(\"task_plans\") or []\n",
        "        return [(int(s.get(\"step_id\", i+1)), str(s.get(\"task_plan\",\"\"))) for i, s in enumerate(steps)]\n",
        "\n",
        "    s = str(plan_result).strip()\n",
        "    try:\n",
        "        obj = json.loads(s)\n",
        "    except Exception:\n",
        "        obj = ast.literal_eval(s)\n",
        "\n",
        "    steps = obj.get(\"task_plan\") or obj.get(\"task_plans\") or []\n",
        "    return [(int(s.get(\"step_id\", i+1)), str(s.get(\"task_plan\",\"\"))) for i, s in enumerate(steps)]\n",
        "\n",
        "print(extract_steps(plan_result))\n",
        "\n"
      ],
      "metadata": {
        "id": "RgWLz0PsVV9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 여기에 state 넣으면 됨\n",
        "obs_map = {\n",
        "    1: {\n",
        "        \"sidebar\": [{\"id\": \"18555\", \"label\": \"【학생신청】신청함\", \"expanded\": False, \"checked\": False}]\n",
        "    },\n",
        "    2: {\n",
        "        \"sidebar\": [{\"id\": \"18555\", \"label\": \"【학생신청】신청함\", \"expanded\": False, \"checked\": True}]\n",
        "    },\n",
        "    3: {\n",
        "        \"sidebar\": [{\"id\": \"fc\", \"label\": \"수업/강의평가\", \"expanded\": True, \"checked\": True, \"sub_items\": [{\"id\": \"fm\", \"label\": \"종합강의시간표조회\", \"checked\": True}]}]\n",
        "    }\n",
        "}\n",
        "\n",
        "def build_action_prompt(user_request: str, step_id: int, task_plan: str, observations: dict | None):\n",
        "    payload = {\n",
        "        \"user_request\": user_request,\n",
        "        \"task_plan\": task_plan,\n",
        "        \"step_id\": str(step_id),\n",
        "    }\n",
        "    if observations is not None:\n",
        "        payload[\"observations\"] = observations\n",
        "    return json.dumps(payload, ensure_ascii=False)\n",
        "\n",
        "def run_actions_from_regex_result(result_list, user_request=\"휴학신청\", obs_map=None, max_new_tokens=256):\n",
        "    outputs = []\n",
        "    for sid, tplan in result_list:\n",
        "        obs = (obs_map or {}).get(sid)\n",
        "        action_prompt = build_action_prompt(user_request, sid, tplan, obs)\n",
        "        out = generate_json(action_prompt, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "        outputs.append((sid, tplan, out))\n",
        "    return outputs\n",
        "\n",
        "# ---- 실행 ----\n",
        "actions = run_actions_from_regex_result(extract_steps(plan_result), user_request=\"휴학신청\", obs_map=obs_map, max_new_tokens=256)\n",
        "\n",
        "for sid, tplan, action in actions:\n",
        "    print(\"------------------------------------------------\")\n",
        "    print(f\"[STEP {sid}] {tplan}\")\n",
        "    print(\"[ACTION RESULT]\\n\", action)\n"
      ],
      "metadata": {
        "id": "-l4lzkm5W4zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()\n"
      ],
      "metadata": {
        "id": "qP10QswjdIzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from huggingface_hub import HfApi, create_repo, upload_folder\n",
        "\n",
        "BASE_REPO = \"leeChaerin/PlanModel_v1\"      # 학습 때 사용한 베이스\n",
        "ADAPTER_DIR = \"./adapters/epoch_009\"       # 저장된 LoRA 어댑터 폴더\n",
        "HF_REPO_ID = \"leeChaerin/ActionModel_v5\"  # 새로 만들 허깅페이스 repo 이름\n",
        "\n",
        "# (1) 허브에 리포지토리 생성(이미 있으면 skip=True)\n",
        "create_repo(repo_id=HF_REPO_ID, private=False, exist_ok=True)\n",
        "\n",
        "# (2) 어댑터 폴더 업로드\n",
        "upload_folder(\n",
        "    repo_id=HF_REPO_ID,\n",
        "    folder_path=ADAPTER_DIR,\n",
        "    path_in_repo=\".\", # 루트에 업로드\n",
        ")\n",
        "print(f\"[OK] LoRA adapter uploaded -> https://huggingface.co/{HF_REPO_ID}\")\n"
      ],
      "metadata": {
        "id": "AL2_bt0odOsg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMqCDYlzzMJQtlvFJ53g1ab",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}