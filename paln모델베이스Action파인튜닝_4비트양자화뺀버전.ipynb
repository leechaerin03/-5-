{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leechaerin03/-5-/blob/main/paln%EB%AA%A8%EB%8D%B8%EB%B2%A0%EC%9D%B4%EC%8A%A4Action%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D_4%EB%B9%84%ED%8A%B8%EC%96%91%EC%9E%90%ED%99%94%EB%BA%80%EB%B2%84%EC%A0%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zBl9BsSuhc-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wandb"
      ],
      "metadata": {
        "id": "dHA8bmWriZmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# LLM ëª¨ë‹ˆí„°ë§\n",
        "wandb.login(key=\"9ed675d942c745bf3fdf524bba1fcd137e6e81ab\")\n",
        "run = wandb.init(project='Fine tuning  LAM2', job_type='training', anonymous='allow')"
      ],
      "metadata": {
        "id": "R16JHGGki3DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gagDJf1PwAMi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    LoraConfig\n",
        ")\n",
        "\n",
        "\n",
        "# --- 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ID ---\n",
        "hub_repo_name = \"leeChaerin/PlanModel_v1\"\n",
        "\n",
        "\n",
        "# --- 3. ëª¨ë¸ ë¡œë“œ (ìˆ˜ì •ë¨) ---\n",
        "# 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •ì„ ì ìš©í•˜ê³ , .to(\"cuda\") ëŒ€ì‹  device_map=\"auto\" ì‚¬ìš©\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    hub_repo_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# --- 4. í† í¬ë‚˜ì´ì € ë¡œë“œ (ìˆ˜ì •ë¨) ---\n",
        "# padding_sideëŠ” ê·¸ëŒ€ë¡œ ë‘ë˜, pad_tokenì€ Noneì¼ ê²½ìš°ì—ë§Œ ì„¤ì •\n",
        "tokenizer = AutoTokenizer.from_pretrained(hub_repo_name, padding_side=\"left\")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"Tokenizer pad_tokenì´ eos_tokenìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# --- 5. (â˜…í•„ìˆ˜â˜…) LoRA ì–´ëŒ‘í„° ì ìš© ---\n",
        "# ì´ ë¶€ë¶„ì´ '...does not require grad...' ì—ëŸ¬ë¥¼ í•´ê²°í•©ë‹ˆë‹¤.\n",
        "# 4ë¹„íŠ¸ë¡œ 'ë™ê²°ëœ' ëª¨ë¸ ìœ„ì— í•™ìŠµ ê°€ëŠ¥í•œ ìƒˆ ë ˆì´ì–´(ì–´ëŒ‘í„°)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], # PlanModel_v1ì˜ ê¸°ë°˜ ëª¨ë¸(Kanana) ê¸°ì¤€\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ì— LoRA ì–´ëŒ‘í„° ì ìš©\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# --- 6. ì¤€ë¹„ ì™„ë£Œ ---\n",
        "print(\"\\n--- LoRA ì–´ëŒ‘í„° ì ìš© í›„ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ---\")\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIi-EnI8xxlT"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(tokenizer.eos_token)\n",
        "#baseëª¨ë¸ì€ eos í† í°ì´ 128001ì„ ì™œ ì°¨ì´ë‚˜ëŠ”ì§„ ëª¨ë¥´ê² ìŒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WmRs0_YyBGN"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"\"\"You are a specialized AI agent for Dongguk Univ nDRIMS. Your task is to analyze the user's input structure and perform one of two specific tasks:\n",
        "    1.  **Plan Generation:**\n",
        "      If the user 'prompt' contains **only** a `user_request` (e.g., \"ë¶ˆêµë™ì•„ë¦¬ ê°€ì…\"), this is a request for a plan.\n",
        "      You MUST respond with the complete `{\"task_plan\": [{\"step_id\": , \"description\":}...]}` JSON for the entire task.\n",
        "      If the user requests a specific course, include the specific course name in the plan description.\n",
        "\n",
        "    2.  **Action Generation:**\n",
        "      If the user 'prompt' contains a `task_plan` (current step) AND `observations` (current state), this is a request for an action.\n",
        "      You MUST respond with the single `{\"action\": {...}}` JSON for that specific step.\n",
        "      For input actions, ensure the `value` field matches the specific data from the plan.\"\"\"}\n",
        "]\n",
        "\n",
        "tokens = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "print(tokens)\n",
        "#print(tokenizer.encode(tokens, add_special_tokens=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_TEXT = (\n",
        "    \"\"\"You are a specialized AI agent for Dongguk Univ nDRIMS. Your task is to analyze the user's input structure and perform one of two specific tasks:\n",
        "    1.  **Plan Generation:**\n",
        "      If the user 'prompt' contains **only** a `user_request` (e.g., \"ë¶ˆêµë™ì•„ë¦¬ ê°€ì…\"), this is a request for a plan.\n",
        "      You MUST respond with the complete `{\"task_plans\": [...]}` JSON for the entire task.\n",
        "\n",
        "    2.  **Action Generation:**\n",
        "      If the user 'prompt' contains a `task_plan` (current step) AND `observations` (current state), this is a request for an action.\n",
        "      You MUST respond with the single `{\"action\": {...}}` JSON for that specific step.\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "LbyCSwm8QF8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5XenGuZyMNh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "print(\"train_data.jsonl ì—…ë¡œë“œ...\")\n",
        "uploaded = files.upload()\n",
        "DATA_PATH = list(uploaded.keys())[0]\n",
        "\n",
        "fixed_data = []\n",
        "print(\"ë°ì´í„°ë¥¼ ë¡œë“œí•˜ë©° íƒ€ì…ì„ ë¬¸ìì—´ë¡œ í†µì¼í•©ë‹ˆë‹¤...\")\n",
        "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            row = json.loads(line)\n",
        "\n",
        "            if isinstance(row.get('prompt'), dict):\n",
        "                row['prompt'] = json.dumps(row['prompt'], ensure_ascii=False)\n",
        "\n",
        "            if isinstance(row.get('output'), dict):\n",
        "                row['output'] = json.dumps(row['output'], ensure_ascii=False)\n",
        "\n",
        "            if 'prompt' in row and 'output' in row:\n",
        "                fixed_data.append(row)\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"JSON íŒŒì‹± ì˜¤ë¥˜ ë¬´ì‹œ: {line}\")\n",
        "\n",
        "raw = Dataset.from_list(fixed_data)\n",
        "\n",
        "print(\"\\n--- ìˆ˜ì •ëœ ë°ì´í„°ë¡œ ë¡œë“œ ì„±ê³µ! ---\")\n",
        "print(raw)\n",
        "print(\"\\n[Row 0 ì˜ˆì‹œ] (Query -> Plan)\")\n",
        "print(raw[0])\n",
        "print(\"\\n[Row 1 ì˜ˆì‹œ] (Plan+State -> Action)\")\n",
        "print(raw[1])\n",
        "print(f\"\\nì´ {len(raw)}ê°œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGl1PAKbyQcy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "qna_list = []\n",
        "\n",
        "SYSTEM_TEXT = (\n",
        "     \"\"\"You are a specialized AI agent for Dongguk Univ nDRIMS. Your task is to analyze the user's input structure and perform one of two specific tasks:\n",
        "    1.  **Plan Generation:**\n",
        "      If the user 'prompt' contains **only** a `user_request` (e.g., \"ë¶ˆêµë™ì•„ë¦¬ ê°€ì…\"), this is a request for a plan.\n",
        "      You MUST respond with the complete `{\"task_plan\": [{\"step_id\": , \"description\":}...]}` JSON for the entire task.\n",
        "      If the user requests a specific course, include the specific course name in the plan description.\n",
        "\n",
        "    2.  **Action Generation:**\n",
        "      If the user 'prompt' contains a `task_plan` (current step) AND `observations` (current state), this is a request for an action.\n",
        "      You MUST respond with the single `{\"action\": {...}}` JSON for that specific step.\n",
        "      For input actions, ensure the `value` field matches the specific data from the plan.\"\"\"\n",
        ")\n",
        "\n",
        "for i in range(len(raw)):\n",
        "    state = raw[i][\"prompt\"]   # ì‚¬ìš©ì ì…ë ¥(í”„ë¡¬í”„íŠ¸)\n",
        "    action = raw[i][\"output\"]  # ì •ë‹µ(assistant ì‘ë‹µ)\n",
        "\n",
        "    # 1) messagesì—ëŠ” system + userë§Œ!\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_TEXT},\n",
        "        {\"role\": \"user\",   \"content\": state},\n",
        "    ]\n",
        "\n",
        "    # 2) ë§ˆì§€ë§‰ userê¹Œì§€ í…œí”Œë¦¿ ì ìš© + assistantê°€ ìƒì„±ë  ìë¦¬ ì—´ê¸°\n",
        "    prompt_ids = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=True, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # 3) ì •ë‹µ í† í°ì€ ë³„ë„ë¡œ ì¸ì½”ë”©í•´ì„œ ë’¤ì— ë¶™ì„ (eos ê¶Œì¥)\n",
        "    answer_ids = tokenizer.encode(\n",
        "        (action or \"\") + (tokenizer.eos_token or \"\"),\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "\n",
        "    # 4) concat\n",
        "    input_ids = prompt_ids + answer_ids\n",
        "\n",
        "    # (ì„ íƒ) í‰ê°€/ë””ë²„ê·¸ìš©ìœ¼ë¡œ ì›ë¬¸ ë¬¸ìì—´ë„ ë³´ê´€í•˜ê³  ì‹¶ë‹¤ë©´:\n",
        "    prompt_str = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    answer_str = ((action or \"\") + (tokenizer.eos_token or \"\"))\n",
        "    input_str  = prompt_str + answer_str\n",
        "\n",
        "    qna_list.append({\n",
        "        \"prompt_ids\": prompt_ids,\n",
        "        \"answer_ids\": answer_ids,\n",
        "        \"input_ids\":  input_ids,\n",
        "        \"prompt_str\": prompt_str,\n",
        "        \"answer_str\": answer_str,\n",
        "        \"input_str\":  input_str,\n",
        "    })\n",
        "\n",
        "# ê¸¸ì´ í†µê³„(íŒ¨ë”© ì „ì— ì°¸ê³ ìš©)\n",
        "max_length = max(len(x[\"input_ids\"]) for x in qna_list)\n",
        "print(\"samples:\", len(qna_list))\n",
        "print(\"max_length:\", max_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1HAKB6GyUBe"
      },
      "outputs": [],
      "source": [
        "pad_id = tokenizer.pad_token_id\n",
        "assert pad_id is not None, \"pad_tokenì´ ì„¤ì •ë¼ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\"\n",
        "\n",
        "class SFTDataset(Dataset):\n",
        "    def __init__(self, items, max_length):\n",
        "        self.inputs = []\n",
        "        self.attns  = []\n",
        "        self.labels = []\n",
        "\n",
        "        for ex in items:\n",
        "            ids = ex[\"input_ids\"]\n",
        "\n",
        "            # íŠ¸ë ì¼€ì´ì…˜/íŒ¨ë”©\n",
        "            if len(ids) > max_length:\n",
        "                ids = ids[-max_length:]  # ë’¤ë¥¼ ë³´ì¡´(ì •ë‹µì´ ë’¤ì— ìˆìœ¼ë¯€ë¡œ)\n",
        "\n",
        "            attn = [1] * len(ids)\n",
        "            pad_len = max_length - len(ids)\n",
        "            if pad_len > 0:\n",
        "                ids  = ids  + [pad_id] * pad_len\n",
        "                attn = attn + [0]      * pad_len\n",
        "\n",
        "            # ë ˆì´ë¸”: í”„ë¡¬í”„íŠ¸ëŠ” -100, ì •ë‹µë§Œ ë ˆì´ë¸”\n",
        "            # ì •ë‹µ ì‹œì‘ ìœ„ì¹˜ = í”„ë¡¬í”„íŠ¸ ê¸¸ì´\n",
        "            prompt_len = len(ex[\"prompt_ids\"])\n",
        "            # (íŠ¸ë ì¼€ì´ì…˜ì´ ìˆì—ˆë‹¤ë©´ prompt_lenë„ ì¬ê³„ì‚°ì´ í•„ìš”í•˜ì§€ë§Œ\n",
        "            # ìœ„ì—ì„œ ë’¤ë¥¼ ë‚¨ê²¼ìœ¼ë‹ˆ ë³´í†µ prompt_len <= max_lengthë¥¼ ë§Œì¡±)\n",
        "            labels = [-100] * len(ids)\n",
        "            for pos in range(prompt_len, len(ex[\"input_ids\"])):\n",
        "                if pos >= max_length:\n",
        "                    break\n",
        "                labels[pos] = ids[pos]\n",
        "\n",
        "            self.inputs.append(torch.tensor(ids))\n",
        "            self.attns.append(torch.tensor(attn))\n",
        "            self.labels.append(torch.tensor(labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.attns[idx], self.labels[idx]\n",
        "\n",
        "dataset = SFTDataset(qna_list, max_length=min(1024, max_length))\n",
        "loader  = DataLoader(dataset, batch_size=2, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTbJQvKDyXAF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "#device = \"cpu\"\n",
        "torch.manual_seed(123)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVnOWDPvya2y"
      },
      "outputs": [],
      "source": [
        "# íŒŒì¸íŠœë‹ ì „ì— ì–´ë–»ê²Œ ëŒ€ë‹µí•˜ëŠ”ì§€ í™•ì¸\n",
        "questions = [ qna['q_ids'] for qna in qna_list]\n",
        "\n",
        "for i, q_ids in enumerate(questions):\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            torch.tensor([q_ids]).to(\"cuda\"),\n",
        "            max_new_tokens=100,\n",
        "            #attention_mask = (input_ids != 0).long(),\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False,\n",
        "            # temperature=1.2,\n",
        "            # top_k=5\n",
        "        )\n",
        "\n",
        "    output_list = output.tolist()\n",
        "\n",
        "    print(f\"Q{i}: {tokenizer.decode(output[0], skip_special_tokens=True)}\")\n",
        "    print(\"----------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhkXO35k-AFZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZKK9p69zoBk"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "for epoch in range(10):\n",
        "    total = 0.0\n",
        "    for input_ids, attn_mask, labels in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attn_mask = attn_mask.to(device)\n",
        "        labels    = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
        "        loss = out.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "        wandb.log({\n",
        "            \"train/loss_step\": loss.item(),\n",
        "            \"epoch\": epoch,\n",
        "            \"step\": global_step,\n",
        "        })\n",
        "\n",
        "    avg_loss = total / len(loader)\n",
        "    losses.append(avg_loss)\n",
        "    print(f\"epoch {epoch} | loss {avg_loss:.4f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"train/loss_epoch\": avg_loss,\n",
        "        \"epoch\": epoch,\n",
        "    })\n",
        "\n",
        "    # â˜… LoRA ì–´ëŒ‘í„° ì €ì¥ (ì¤‘ìš”)\n",
        "    save_dir = f\"./adapters/epoch_{epoch:03d}\"\n",
        "    model.save_pretrained(save_dir)      # â† ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ ì €ì¥\n",
        "    # (ì„ íƒ) í† í¬ë‚˜ì´ì €ë„ í•¨ê»˜ ë³´ê´€í•˜ë©´ í¸í•¨\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(f\"[saved] LoRA adapter -> {save_dir}\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import IFrame, display\n",
        "\n",
        "\n",
        "display(IFrame(run.url, width=1200, height=720))\n"
      ],
      "metadata": {
        "id": "lc2pjKL2mDa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt1lBgeZLopX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_repo = \"leeChaerin/PlanModel_v1\"\n",
        "adapter_path = \"./adapters/epoch_009\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_repo)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_repo,\n",
        "    torch_dtype=torch.float16,   # ë˜ëŠ” float32 (CPUë©´)\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "model.eval()\n",
        "\n",
        "print(\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxJ5ujNzPV7i"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(range(1, len(losses)+1), losses, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, json, torch\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "SYSTEM_TEXT = (\n",
        "     \"\"\"You are a specialized AI agent for Dongguk Univ nDRIMS. Your task is to analyze the user's input structure and perform one of two specific tasks:\n",
        "    1.  **Plan Generation:**\n",
        "      If the user 'prompt' contains **only** a `user_request` (e.g., \"ë¶ˆêµë™ì•„ë¦¬ ê°€ì…\"), this is a request for a plan.\n",
        "      You MUST respond with the complete `{\"task_plan\": [{\"step_id\": , \"description\":}...]}` JSON for the entire task.\n",
        "      If the user requests a specific course, include the specific course name in the plan description.\n",
        "\n",
        "    2.  **Action Generation:**\n",
        "      If the user 'prompt' contains a `task_plan` (current step) AND `observations` (current state), this is a request for an action.\n",
        "      You MUST respond with the single `{\"action\": {...}}` JSON for that specific step.\n",
        "      For input actions, ensure the `value` field matches the specific data from the plan.\"\"\"\n",
        ")\n",
        "\n",
        "def build_inputs(user_prompt: str):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_TEXT},\n",
        "        {\"role\": \"user\",   \"content\": user_prompt},\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    return {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "# 1) ê· í˜• ê²€ì‚¬ ìœ í‹¸\n",
        "def is_balanced_json_fragment(s: str) -> bool:\n",
        "    in_str = False\n",
        "    esc = False\n",
        "    stack = []\n",
        "    for ch in s:\n",
        "        if in_str:\n",
        "            if esc:\n",
        "                esc = False\n",
        "            elif ch == '\\\\':\n",
        "                esc = True\n",
        "            elif ch == '\"':\n",
        "                in_str = False\n",
        "            continue\n",
        "        else:\n",
        "            if ch == '\"':\n",
        "                in_str = True\n",
        "            elif ch == '{':\n",
        "                stack.append('}')\n",
        "            elif ch == '[':\n",
        "                stack.append(']')\n",
        "            elif ch in ('}', ']'):\n",
        "                if not stack or stack[-1] != ch:\n",
        "                    return False\n",
        "                stack.pop()\n",
        "    return len(stack) == 0 and len(s) > 0\n",
        "\n",
        "# 2) ê°€ì¥ ê¸´ ê· í˜• ì¡íŒ JSON ë©ì–´ë¦¬ ì¶”ì¶œ\n",
        "def longest_balanced_json(text: str) -> str | None:\n",
        "    # ì²« { ë˜ëŠ” [ ë¶€í„° ìŠ¤ìº”\n",
        "    start = None\n",
        "    for i, ch in enumerate(text):\n",
        "        if ch in '{[':\n",
        "            start = i\n",
        "            break\n",
        "    if start is None:\n",
        "        return None\n",
        "\n",
        "    in_str = False\n",
        "    esc = False\n",
        "    stack = []\n",
        "    last_good = None\n",
        "    for i, ch in enumerate(text[start:], start=start):\n",
        "        if in_str:\n",
        "            if esc:\n",
        "                esc = False\n",
        "            elif ch == '\\\\':\n",
        "                esc = True\n",
        "            elif ch == '\"':\n",
        "                in_str = False\n",
        "        else:\n",
        "            if ch == '\"':\n",
        "                in_str = True\n",
        "            elif ch == '{':\n",
        "                stack.append('}')\n",
        "            elif ch == '[':\n",
        "                stack.append(']')\n",
        "            elif ch in ('}', ']'):\n",
        "                if not stack or stack[-1] != ch:\n",
        "                    break\n",
        "                stack.pop()\n",
        "                if not stack:\n",
        "                    last_good = i + 1\n",
        "    return text[start:last_good].strip() if last_good else None\n",
        "\n",
        "# 3) ìŠ¤í†± ì¡°ê±´ êµì²´: ë‹¨ì¼ '}' ëŒ€ì‹  'ê· í˜• JSONì´ë©´ ë©ˆì¶¤'\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "import re, torch\n",
        "\n",
        "class StopOnBalancedJSON(StoppingCriteria):\n",
        "    def __init__(self, tokenizer, lookback_tokens=400):\n",
        "        self.tok = tokenizer\n",
        "        self.lookback_tokens = lookback_tokens\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        txt = self.tok.decode(input_ids[0][-self.lookback_tokens:], skip_special_tokens=True)\n",
        "        # ë§ˆì§€ë§‰ JSON ì‹œì‘ í›„ë³´ë¥¼ ëŒ€ëµ ì¡ì•„ ê· í˜• í™•ì¸\n",
        "        m = re.search(r'([\\{\\[].*)$', txt, flags=re.DOTALL)\n",
        "        cand = m.group(1) if m else txt\n",
        "        return is_balanced_json_fragment(cand)\n",
        "\n",
        "stoppers = StoppingCriteriaList([StopOnBalancedJSON(tokenizer)])\n",
        "# 4) generate_json í›„ì²˜ë¦¬ êµì²´: ì •ê·œì‹ candidates ëŒ€ì‹  longest_balanced_json ì‚¬ìš©\n",
        "def generate_json(user_prompt: str, max_new_tokens=256, do_sample=False):\n",
        "    inputs = build_inputs(user_prompt)\n",
        "    input_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=do_sample,\n",
        "            temperature=0.7 if do_sample else None,\n",
        "            top_p=0.9 if do_sample else None,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            stopping_criteria=stoppers,\n",
        "        )\n",
        "\n",
        "    # ì…ë ¥ ê¸¸ì´ ì´í›„(ì‹ ê·œ ìƒì„±ë¶„)ë§Œ ë””ì½”ë”©\n",
        "    new_tokens = out_ids[0, input_len:]\n",
        "    text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "    # ê· í˜• ì¡íŒ JSONë§Œ ì˜ë¼ì„œ íŒŒì‹±\n",
        "    jb = longest_balanced_json(text)\n",
        "    if jb is None:\n",
        "        return text  # ë””ë²„ê·¸ìš©ìœ¼ë¡œ ì›ë¬¸ ë°˜í™˜\n",
        "    try:\n",
        "        return json.loads(jb)\n",
        "    except Exception:\n",
        "        return jb  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬¸ìì—´ë¡œ ë°˜í™˜(ë””ë²„ê·¸)\n"
      ],
      "metadata": {
        "id": "CBCZBaPBLRms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- í…ŒìŠ¤íŠ¸ ----\n",
        "plan_prompt = \"{'user_request': 'ìí‡´ì‹ ì²­'}\"\n",
        "plan_result = generate_json(plan_prompt)\n",
        "print(plan_result)"
      ],
      "metadata": {
        "id": "9jISvRcOXfYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------ëª¨ë¸ ì“°ëŠ” ë°©ë²• ê°€ì´ë“œ----------\n",
        "\n",
        "# 1. ì‚¬ìš©ì ì…ë ¥ì„ ë„£ê³ , planì„ ìƒì„±\n",
        "import json\n",
        "\n",
        "user_req = input(\"ë¬´ì—‡ì„ í•˜ê³  ì‹¶ë‚˜ìš”? (ì˜ˆ: íœ´í•™ì‹ ì²­): \").strip()\n",
        "\n",
        "plan_payload = {\"user_request\": user_req}\n",
        "plan_prompt = json.dumps(plan_payload, ensure_ascii=False)\n",
        "\n",
        "plan_result = generate_json(plan_prompt, do_sample=False)\n",
        "print(\"[PLAN RESULT]\")\n",
        "print(plan_result)"
      ],
      "metadata": {
        "id": "U9zLvYrpdzCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, ast\n",
        "\n",
        "def extract_steps(plan_result):\n",
        "    # 1) dictë©´ ë°”ë¡œ ì‚¬ìš©\n",
        "    if isinstance(plan_result, dict):\n",
        "        steps = plan_result.get(\"task_plan\") or plan_result.get(\"task_plans\") or []\n",
        "        return [(int(s.get(\"step_id\", i+1)), str(s.get(\"task_plan\",\"\"))) for i, s in enumerate(steps)]\n",
        "\n",
        "    # 2) ë¬¸ìì—´ì´ë©´ JSON â†’ ì‹¤íŒ¨ ì‹œ literal_eval ìˆœì„œë¡œ íŒŒì‹±\n",
        "    s = str(plan_result).strip()\n",
        "    try:\n",
        "        obj = json.loads(s)\n",
        "    except Exception:\n",
        "        obj = ast.literal_eval(s)  # '...' ìŠ¤íƒ€ì¼ íŒŒì‹±\n",
        "\n",
        "    steps = obj.get(\"task_plan\") or obj.get(\"task_plans\") or []\n",
        "    return [(int(s.get(\"step_id\", i+1)), str(s.get(\"task_plan\",\"\"))) for i, s in enumerate(steps)]\n",
        "\n",
        "print(extract_steps(plan_result))\n",
        "\n"
      ],
      "metadata": {
        "id": "RgWLz0PsVV9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 1) ì •ê·œì‹ìœ¼ë¡œ ë½‘ì€ ê²°ê³¼ ì˜ˆì‹œ (ì´ë¯¸ ë„ˆê°€ ë§Œë“  result)\n",
        "# result = [(1, \"ì¢Œì¸¡ ë©”ë‰´ì—ì„œ 'ã€í•™ìƒì‹ ì²­ã€‘ì‹ ì²­í•¨'ë¥¼ í´ë¦­í•œë‹¤.\"), (2, \"...\"), (3, \"...\")]\n",
        "\n",
        "# ì—¬ê¸°ì— state ë„£ìœ¼ë©´ ë¨\n",
        "obs_map = {\n",
        "    1: {\n",
        "        \"sidebar\": [{\"id\": \"18555\", \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì‹ ì²­í•¨\", \"expanded\": False, \"checked\": False}]\n",
        "    },\n",
        "    2: {\n",
        "        \"sidebar\": [{\"id\": \"18555\", \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì‹ ì²­í•¨\", \"expanded\": False, \"checked\": True}]\n",
        "    },\n",
        "    3: {\n",
        "        \"sidebar\": [{\"id\": \"fc\", \"label\": \"ìˆ˜ì—…/ê°•ì˜í‰ê°€\", \"expanded\": True, \"checked\": True, \"sub_items\": [{\"id\": \"fm\", \"label\": \"ì¢…í•©ê°•ì˜ì‹œê°„í‘œì¡°íšŒ\", \"checked\": True}]}]\n",
        "    }\n",
        "}\n",
        "\n",
        "def build_action_prompt(user_request: str, step_id: int, task_plan: str, observations: dict | None):\n",
        "    payload = {\n",
        "        \"user_request\": user_request,\n",
        "        \"task_plan\": task_plan,\n",
        "        \"step_id\": str(step_id),\n",
        "    }\n",
        "    if observations is not None:\n",
        "        payload[\"observations\"] = observations\n",
        "    # ë¬¸ìì—´ ê¹¨ì§ ë°©ì§€: ë°˜ë“œì‹œ json.dumps ì‚¬ìš©\n",
        "    return json.dumps(payload, ensure_ascii=False)\n",
        "\n",
        "def run_actions_from_regex_result(result_list, user_request=\"íœ´í•™ì‹ ì²­\", obs_map=None, max_new_tokens=256):\n",
        "    outputs = []\n",
        "    for sid, tplan in result_list:\n",
        "        obs = (obs_map or {}).get(sid)\n",
        "        action_prompt = build_action_prompt(user_request, sid, tplan, obs)\n",
        "        out = generate_json(action_prompt, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "        outputs.append((sid, tplan, out))\n",
        "    return outputs\n",
        "\n",
        "# ---- ì‹¤í–‰ ----\n",
        "actions = run_actions_from_regex_result(extract_steps(plan_result), user_request=\"íœ´í•™ì‹ ì²­\", obs_map=obs_map, max_new_tokens=256)\n",
        "\n",
        "for sid, tplan, action in actions:\n",
        "    print(\"------------------------------------------------\")\n",
        "    print(f\"[STEP {sid}] {tplan}\")\n",
        "    print(\"[ACTION RESULT]\\n\", action)\n"
      ],
      "metadata": {
        "id": "-l4lzkm5W4zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()  # â–¶ í—ˆê¹…í˜ì´ìŠ¤ í† í° ì…ë ¥ (Settings > Access Tokensì—ì„œ ìƒì„±í•œ í† í°)\n"
      ],
      "metadata": {
        "id": "qP10QswjdIzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from huggingface_hub import HfApi, create_repo, upload_folder\n",
        "\n",
        "BASE_REPO = \"leeChaerin/PlanModel_v1\"      # í•™ìŠµ ë•Œ ì‚¬ìš©í•œ ë² ì´ìŠ¤\n",
        "ADAPTER_DIR = \"./adapters/epoch_009\"       # ì €ì¥ëœ LoRA ì–´ëŒ‘í„° í´ë”\n",
        "HF_REPO_ID = \"leeChaerin/ActionModel_v5\"  # ìƒˆë¡œ ë§Œë“¤ í—ˆê¹…í˜ì´ìŠ¤ repo ì´ë¦„\n",
        "\n",
        "# (1) í—ˆë¸Œì— ë¦¬í¬ì§€í† ë¦¬ ìƒì„±(ì´ë¯¸ ìˆìœ¼ë©´ skip=True)\n",
        "create_repo(repo_id=HF_REPO_ID, private=False, exist_ok=True)\n",
        "\n",
        "# (2) ì–´ëŒ‘í„° í´ë” ì—…ë¡œë“œ\n",
        "upload_folder(\n",
        "    repo_id=HF_REPO_ID,\n",
        "    folder_path=ADAPTER_DIR,\n",
        "    path_in_repo=\".\", # ë£¨íŠ¸ì— ì—…ë¡œë“œ\n",
        ")\n",
        "print(f\"[OK] LoRA adapter uploaded -> https://huggingface.co/{HF_REPO_ID}\")\n"
      ],
      "metadata": {
        "id": "AL2_bt0odOsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_prompt = \"\"\"{\n",
        " 'user_request': 'íœ´í•™ì‹ ì²­',\n",
        " 'task_plan': \"ì¢Œì¸¡ ë©”ë‰´ì—ì„œ 'í•™ìƒì‹ ì²­(ê¸°íƒ€)'ë¥¼ í¼ì¹œë‹¤\",\n",
        " 'step_id': '1',\n",
        " 'observations': {'sidebar': [{'id':'d7','label':'í•™ìƒì‹ ì²­(ê¸°íƒ€)','expanded': false}]}]}\n",
        "}\"\"\"\n",
        "print(generate_json(action_prompt))"
      ],
      "metadata": {
        "id": "VCX6O6OgSXQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, json, re\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "BASE_REPO   = \"leeChaerin/PlanModel_v1\"   # í•™ìŠµ ì‹œ ì¼ë˜ ë² ì´ìŠ¤\n",
        "ADAPTER_DIR = \"./adapters/epoch_002\"      # ì €ì¥í•´ë‘” LoRA ì–´ëŒ‘í„° í´ë”ë¡œ ë³€ê²½\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_REPO, padding_side=\"left\", trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_REPO,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
        "model.eval()\n",
        "print(\"[OK] Adapter loaded:\", ADAPTER_DIR)\n"
      ],
      "metadata": {
        "id": "8DLasx0x7k4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install outlines"
      ],
      "metadata": {
        "id": "EC_iDawZ-fGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í˜•ì‹ ê³ ì •í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë„£ê¸°\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Dict, Any, Literal\n",
        "from outlines.models.transformers import Transformers\n",
        "from outlines import Generator\n",
        "\n",
        "# 'args'ì— ë“¤ì–´ê°ˆ ë‚´ìš© ì •ì˜\n",
        "class ClickArgs(BaseModel):\n",
        "    role: str\n",
        "    aria_label: str = Field(alias=\"aria-label\")\n",
        "\n",
        "# ìµœì¢… ì¶œë ¥ JSON êµ¬ì¡° ì •ì˜\n",
        "class ActionModel(BaseModel):\n",
        "    function: str\n",
        "    control_label: str\n",
        "    control_type: str\n",
        "    args: ClickArgs\n",
        "    status: Literal[\"CONTINUE\", \"FINISH\"]\n",
        "\n",
        "\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "hub_repo_name = \"leeChaerin/PlanModel_v1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    hub_repo_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "# í† í¬ë‚˜ì´ì €ë„ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.\n",
        "tokenizer = AutoTokenizer.from_pretrained(hub_repo_name)\n",
        "\n",
        "adapter_path = \"./model_epoch_009\"\n",
        "\n",
        "# 'model' ê°ì²´ê°€ ì—¬ê¸°ì„œ ìƒˆë¡œ ìƒì„±ë©ë‹ˆë‹¤.\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "# --- (C) ì¶”ë¡  ì¤€ë¹„ ---\n",
        "model.eval()\n",
        "print(f\"'{adapter_path}'ì—ì„œ ì–´ëŒ‘í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "outlines_model = Transformers(model=model, tokenizer=tokenizer)\n",
        "action_generator = Generator(outlines_model, ActionModel)\n",
        "\n",
        "user_input = input(\"ìš”ì²­ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"You are a helpful AI assistant for Dongguk Univ nDRIMS. Your job is to understand the user's request, formulate a step-by-step plan, and then output the correct action based on the current system observations (state).\"},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "    {'user_request': '{user_input}',\n",
        "     'observations': {'sidebar': [{'id': 'd7', 'label': 'í•™ìƒì‹ ì²­(ê¸°íƒ€)', 'expanded': false},{'id': 'e5', 'label': 'ê³µì§€ì‚¬í•­', 'expanded': true, 'sub_items': [{'id': 'lq', 'label': 'ê³µì§€ì‚¬í•­ì¡°íšŒ', 'checked': true}]}]}\n",
        "    \"\"\"},\n",
        "]\n",
        "\n",
        "prompt_string = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True  # '[ASSISTANT]' í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì¶”ê°€\n",
        ")\n",
        "\n",
        "print(f\"---  Action ìƒì„± ---\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # action_generatorê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ Pydantic ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "    action_result = action_generator(prompt_string, max_new_tokens=256)\n",
        "\n",
        "\n",
        "# --- 3. ê²°ê³¼ ì¶œë ¥ ---\n",
        "# action_resultëŠ” Pydantic ê°ì²´ì´ë¯€ë¡œ, .model_dump_json()ì„ ì‚¬ìš©í•´\n",
        "# ê¹”ë”í•œ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "print(\"\\n--- Outlinesê°€ ê°•ì œí•œ JSON ì¶œë ¥ ---\")\n",
        "try:\n",
        "    print(action_result.model_dump_json(indent=2))\n",
        "except Exception as e:\n",
        "    print(f\"JSON ë³€í™˜ ì˜¤ë¥˜: {e}\")\n",
        "    print(action_result) # Pydantic ê°ì²´ ì›ë³¸ ì¶œë ¥\n",
        "\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"You are a helpful AI assistant for Dongguk Univ nDRIMS. Your job is to understand the user's request, formulate a step-by-step plan, and then output the correct action based on the current system observations (state).\"},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "    {'user_request': '{user_input}',\n",
        "    'observations': {\"sidebar\": [{\"id\": \"d7\", \"label\": \"í•™ìƒì‹ ì²­(ê¸°íƒ€)\", \"expanded\": true, \"sub_items\": [{\"id\": \"ee\", \"label\": \"ë¶ˆêµë™ì•„ë¦¬ê°€ì…\", \"checked\": false}]}]}}}\n",
        "    \"\"\"},\n",
        "]\n",
        "\n",
        "prompt_string = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True  # '[ASSISTANT]' í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì¶”ê°€\n",
        ")\n",
        "\n",
        "print(f\"---  Action2 ìƒì„± ---\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # action_generatorê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ Pydantic ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "    action_result = action_generator(prompt_string, max_new_tokens=256)\n",
        "\n",
        "\n",
        "# --- 3. ê²°ê³¼ ì¶œë ¥ ---\n",
        "# action_resultëŠ” Pydantic ê°ì²´ì´ë¯€ë¡œ, .model_dump_json()ì„ ì‚¬ìš©í•´\n",
        "# ê¹”ë”í•œ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "print(\"\\n--- Outlinesê°€ ê°•ì œí•œ JSON ì¶œë ¥ ---\")\n",
        "try:\n",
        "    print(action_result.model_dump_json(indent=2))\n",
        "except Exception as e:\n",
        "    print(f\"JSON ë³€í™˜ ì˜¤ë¥˜: {e}\")\n",
        "    print(action_result) # Pydantic ê°ì²´ ì›ë³¸ ì¶œë ¥\n"
      ],
      "metadata": {
        "id": "U-9Qz4et82yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_once(user_prompt: str, max_new_tokens=256, do_sample=False, temperature=0.7, top_p=0.9):\n",
        "    inputs = build_inputs(user_prompt)\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=do_sample,\n",
        "            temperature=temperature if do_sample else None,\n",
        "            top_p=top_p if do_sample else None,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "    text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "    return text\n",
        "\n",
        "# ëª¨ë¸ ì¶œë ¥ì—ì„œ ë§ˆì§€ë§‰ assistant ë¶€ë¶„ë§Œ(=ì •ë‹µ JSON) ë½‘ê³  ì‹¶ìœ¼ë©´:\n",
        "ASSISTANT_SPLIT_RE = re.compile(r\"(?:<<assistant>>|assistant\\n|assistant:)\", re.IGNORECASE)\n",
        "\n",
        "def extract_assistant_text(full_text: str):\n",
        "    # í…œí”Œë¦¿ì— ë”°ë¼ ì „ì²´ ëŒ€í™”ê°€ í¬í•¨ë  ìˆ˜ ìˆì–´, ë’¤ìª½ JSONì„ heuristicsë¡œ ì¶”ì¶œ\n",
        "    # ë‹¨ìˆœí™”: ì¤‘ê´„í˜¸ ë¸”ë¡ ì¤‘ ë§ˆì§€ë§‰ JSON ê°ì²´/ë°°ì—´ ì¶”ì¶œ\n",
        "    candidates = re.findall(r\"(\\{.*\\}|\\[.*\\])\", full_text, flags=re.DOTALL)\n",
        "    return candidates[-1].strip() if candidates else full_text.strip()\n",
        "\n",
        "def try_parse_json(s: str):\n",
        "    try:\n",
        "        return json.loads(s), None\n",
        "    except Exception as e:\n",
        "        return None, e\n"
      ],
      "metadata": {
        "id": "BIvroOVULb6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (A) Plan Generation í…ŒìŠ¤íŠ¸\n",
        "plan_prompt = \"{'user_request': 'íœ´í•™ì‹ ì²­'}\"\n",
        "gen_plan = generate_once(plan_prompt, do_sample=False)\n",
        "print(\"\\n[RAW OUTPUT - PLAN]\\n\", gen_plan)\n",
        "\n",
        "only_json = extract_assistant_text(gen_plan)\n",
        "parsed, err = try_parse_json(only_json)\n",
        "print(\"\\n[PARSED JSON - PLAN]\\n\", parsed if err is None else only_json)\n",
        "\n"
      ],
      "metadata": {
        "id": "mQpo5o5FLc7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Dict, Any, Literal, List\n",
        "from outlines.models.transformers import Transformers\n",
        "from outlines import Generator\n",
        "\n",
        "# (ê°€ì •: 'model'ê³¼ 'tokenizer'ëŠ” ì´ì „ ì…€ì—ì„œ ë¡œë“œ ì™„ë£Œë¨)\n",
        "if 'model' not in globals() or 'tokenizer' not in globals():\n",
        "    print(\"ğŸš¨ ì˜¤ë¥˜: 'model' ë˜ëŠ” 'tokenizer'ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"ì´ ì…€ì„ ì‹¤í–‰í•˜ê¸° ì „ì— [14]ë²ˆ ì…€ (ëª¨ë¸ ë¡œë“œ)ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
        "    # Colabì—ì„œ ì…€ ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•\n",
        "    raise NameError(\"'model' or 'tokenizer' not defined.\")\n",
        "\n",
        "\n",
        "# --- 1. Pydantic ìŠ¤í‚¤ë§ˆ 2ê°œ ì •ì˜ ---\n",
        "\n",
        "# â­ï¸ Task A (Plan ìƒì„±)ë¥¼ ìœ„í•œ ìŠ¤í‚¤ë§ˆ\n",
        "class Step(BaseModel):\n",
        "    step_id: int\n",
        "    description: str\n",
        "\n",
        "class TaskPlan(BaseModel):\n",
        "    task_plan: List[Step]\n",
        "\n",
        "# â­ï¸ Task B (Action ìƒì„±)ë¥¼ ìœ„í•œ ìŠ¤í‚¤ë§ˆ\n",
        "class ClickArgs(BaseModel):\n",
        "    role: str\n",
        "    aria_label: str = Field(alias=\"aria-label\")\n",
        "\n",
        "class ActionModel(BaseModel):\n",
        "    function: str\n",
        "    control_label: str\n",
        "    control_type: str\n",
        "    args: ClickArgs\n",
        "    status: Literal[\"CONTINUE\", \"FINISH\"]\n",
        "\n",
        "# --- 2. Outlines ì œë„ˆë ˆì´í„° 2ê°œ ìƒì„± ---\n",
        "# (ë‘ ì œë„ˆë ˆì´í„° ëª¨ë‘ ë™ì¼í•œ 'model' ê°ì²´ë¥¼ ì‚¬ìš©)\n",
        "\n",
        "print(f\"ë˜í•‘í•  ëª¨ë¸ ê°ì²´: {model.__class__.__name__}\") # PeftModelForCausalLM\n",
        "outlines_model = Transformers(model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Task A (Plan) ì œë„ˆë ˆì´í„°\n",
        "plan_generator = Generator(outlines_model, TaskPlan)\n",
        "print(\"âœ… 'Plan ìƒì„±ê¸°' ì¤€ë¹„ ì™„ë£Œ.\")\n",
        "\n",
        "# Task B (Action) ì œë„ˆë ˆì´í„°\n",
        "action_generator = Generator(outlines_model, ActionModel)\n",
        "print(\"âœ… 'Action ìƒì„±ê¸°' ì¤€ë¹„ ì™„ë£Œ.\")\n",
        "\n",
        "# --- 3. âš¡ï¸ ìƒíƒœ(State) ì •ì˜ âš¡ï¸ ---\n",
        "\n",
        "user_query = \"ë¶ˆêµë™ì•„ë¦¬\"\n",
        "\n",
        "# â­ï¸ State 1: 1ë‹¨ê³„ (Plan ì‹¤í–‰ ì „)ì˜ 'ì´ˆê¸° ìƒíƒœ'\n",
        "# (Cell [12]ì˜ ì˜ˆì‹œì™€ ë™ì¼: í•™ìƒì‹ ì²­(ê¸°íƒ€)ê°€ ë‹«í˜€ ìˆìŒ)\n",
        "initial_state_json = {\n",
        "    \"sidebar\": [\n",
        "    {\n",
        "      \"label\": \"ê°œì¸ì •ë³´ìˆ˜ì§‘ë™ì˜\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì‹ ì²­í•¨\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì§„í–‰í•¨\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì™„ë£Œí•¨\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"í•™ìƒì‹ ì²­(ê¸°íƒ€)\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"í•™ì /í™•ì¸ì„œ\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ìˆ˜ê°•ì‹ ì²­\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ìˆ˜ì—…/ê°•ì˜í‰ê°€\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì„±ì \",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¥í•™\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ë“±ë¡\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"êµì§\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¡¸ì—…\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ê³µí•™êµìœ¡ì¸ì¦\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"í˜„ì¥ì‹¤ìŠµ\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì˜ˆë¹„êµ°\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"êµìœ¡ì„¼í„°\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¨ë¨¸ìŠ¤ì¿¨\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ë‚¨ì‚°í•™ì‚¬\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¶©ë¬´í•™ì‚¬\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ê³ ì–‘í•™ì‚¬\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ê³µì§€ì‚¬í•­\",\n",
        "      \"expanded\": True,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": [\n",
        "        {\n",
        "          \"label\": \"ê³µì§€ì‚¬í•­ì¡°íšŒ\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": True,\n",
        "          \"sub_items\": []\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "# â­ï¸ State 2: 1ë‹¨ê³„ Action(\"í•™ìƒì‹ ì²­(ê¸°íƒ€)\" í´ë¦­)ì´ ì‹¤í–‰ëœ 'ì´í›„ì˜ ìƒíƒœ'\n",
        "# (ì „ì²´ JSONìœ¼ë¡œ í™•ì¥ëœ ë²„ì „. Python ë³€ìˆ˜ì´ë¯€ë¡œ true/falseê°€ ì•„ë‹Œ True/False ì‚¬ìš©)\n",
        "state_after_step_1_json = {\n",
        "    \"sidebar\": [\n",
        "    {\n",
        "      \"label\": \"ê°œì¸ì •ë³´ìˆ˜ì§‘ë™ì˜\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì‹ ì²­í•¨\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì§„í–‰í•¨\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì™„ë£Œí•¨\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"í•™ìƒì‹ ì²­(ê¸°íƒ€)\",\n",
        "      \"expanded\": True,\n",
        "      \"checked\": True,\n",
        "      \"sub_items\": [\n",
        "        {\n",
        "          \"label\": \"ë´‰ì‚¬ì†Œê°ë¬¸ë“±ë¡\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"ì‹ í–‰í™œë™ì‹ ì²­\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"ì‹ í–‰í™œë™ë§ˆì¼ë¦¬ì§€ì ìˆ˜í™•ì¸\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"í•™ìƒì‹ í–‰í™œë™ê¸°ì¤€í‘œì•ˆë‚´\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"êµì§ì´ìˆ˜ì‹ ì²­/ì·¨ì†Œì‹ ì²­\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"êµìœ¡ë´‰ì‚¬í™œë™ì‹œê°„ë“±ë¡\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"ì„±ì¸ì§€êµìœ¡ì‹ ì²­\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"ì‘ê¸‰ì²˜ì¹˜ë°ì‹¬íì†Œìƒìˆ  ì‹¤ìŠµì‹ ì²­\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"êµì§ì¸ì ì„±ê²€ì‚¬ ì‹ ì²­\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"êµì§ê¸°íƒ€ ì„œë¹„ìŠ¤ì‹ ì²­\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"DIYì„¤ë¬¸ì¡°ì‚¬\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"êµë¥˜ìƒí™©ë°íŒŒê²¬ì •ë³´ë“±ë¡\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"êµ­ì™¸êµë¥˜ì¸ì •ì„±ì ë“±ë¡\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"í•´ì™¸í•™ìˆ íƒë°©ì¼ì •ê´€ë¦¬\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"ê·¼ë¡œì¥í•™ê·¼ë¡œì‹œê°„ë“±ë¡\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"ë¶ˆêµë™ì•„ë¦¬ê°€ì…\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"í•™ìƒì¦ê°œì¸ì •ë³´ë™ì˜\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"DEISí•™ë¶€ëª¨ì„œë¹„ìŠ¤ì œê³µë™ì˜\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"íˆ¬í‘œí•˜ê¸°\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"í•™ì /í™•ì¸ì„œ\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ìˆ˜ê°•ì‹ ì²­\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ìˆ˜ì—…/ê°•ì˜í‰ê°€\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì„±ì \",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¥í•™\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ë“±ë¡\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"êµì§\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¡¸ì—…\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ê³µí•™êµìœ¡ì¸ì¦\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"í˜„ì¥ì‹¤ìŠµ\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì˜ˆë¹„êµ°\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"êµìœ¡ì„¼í„°\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¨ë¨¸ìŠ¤ì¿¨\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ë‚¨ì‚°í•™ì‚¬\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¶©ë¬´í•™ì‚¬\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ê³ ì–‘í•™ì‚¬\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ê³µì§€ì‚¬í•­\",\n",
        "      \"expanded\": False,\n",
        "      \"checked\": False,\n",
        "      \"sub_items\": [\n",
        "        {\n",
        "          \"label\": \"ê³µì§€ì‚¬í•­ì¡°íšŒ\",\n",
        "          \"expanded\": False,\n",
        "          \"checked\": False,\n",
        "          \"sub_items\": []\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "\n",
        "# --- 4. âš¡ï¸ ìë™í™” íŒŒì´í”„ë¼ì¸(ë£¨í”„) ì‹¤í–‰ âš¡ï¸ ---\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "\n",
        "    # === 1ë‹¨ê³„: Query â¡ï¸ Plan ===\n",
        "    print(f\"\\n--- 1ë‹¨ê³„: '{user_query}'ë¡œ Plan ìƒì„± ---\")\n",
        "\n",
        "    plan_messages = [\n",
        "        {\"role\": \"system\",\n",
        "      \"content\": \"\"\"You are a specialized AI agent for Dongguk Univ nDRIMS. Your task is to analyze the user's input structure and perform one of two specific tasks:\n",
        "      1.  **Plan Generation:**\n",
        "        If the user 'prompt' contains **only** a `user_request` (e.g., \"ë¶ˆêµë™ì•„ë¦¬ ê°€ì…\"), this is a request for a plan.\n",
        "        You MUST respond with the complete `{\"task_plans\": [...]}` JSON for the entire task.\n",
        "\n",
        "      2.  **Action Generation:**\n",
        "        If the user 'prompt' contains a `task_plan` (current step) AND `observations` (current state), this is a request for an action.\n",
        "        You MUST respond with the single `{\"action\": {...}}` JSON for that specific step.\"\"\"},\n",
        "        {\"role\": \"user_request\", \"content\": user_query}\n",
        "    ]\n",
        "    plan_prompt_string = tokenizer.apply_chat_template(\n",
        "        plan_messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    generated_plan_obj = None # Pydantic ê°ì²´ë¥¼ ë‹´ì„ ë³€ìˆ˜\n",
        "    try:\n",
        "        # Pydantic ê°ì²´ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì‹œë„\n",
        "        generated_plan_str_or_obj = plan_generator(plan_prompt_string, max_new_tokens=256)\n",
        "\n",
        "        # Pydantic ê°ì²´ê°€ ë°˜í™˜ë˜ì—ˆëŠ”ì§€, ë¬¸ìì—´ì´ ë°˜í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
        "        if isinstance(generated_plan_str_or_obj, str):\n",
        "            print(f\"âš ï¸ 1ë‹¨ê³„ ê²½ê³ : Outlinesê°€ ë¬¸ìì—´ ë°˜í™˜. ìˆ˜ë™ íŒŒì‹± ì‹œë„...\")\n",
        "            print(f\"   ë°˜í™˜ëœ ì›ë³¸(str): {generated_plan_str_or_obj}\")\n",
        "            json_data = json.loads(generated_plan_str_or_obj)\n",
        "            generated_plan_obj = TaskPlan.model_validate(json_data)\n",
        "        else:\n",
        "            generated_plan_obj = generated_plan_str_or_obj # Pydantic ê°ì²´\n",
        "\n",
        "        print(\"   âœ… 1ë‹¨ê³„ Plan ìƒì„± ì„±ê³µ!\")\n",
        "        print(generated_plan_obj.model_dump_json(indent=2))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ğŸš¨ 1ë‹¨ê³„ ì˜¤ë¥˜: Plan ìƒì„± ë˜ëŠ” íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
        "        # 1ë‹¨ê³„ê°€ ì‹¤íŒ¨í•˜ë©´ 2ë‹¨ê³„ë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì¤‘ë‹¨\n",
        "        raise Exception(\"1ë‹¨ê³„ Plan ìƒì„± ì‹¤íŒ¨\")\n",
        "\n",
        "\n",
        "    # === 2ë‹¨ê³„: (Plan + State) â¡ï¸ Action (ë£¨í”„ ì‹¤í–‰) ===\n",
        "\n",
        "    # â­ï¸ ì²« ë²ˆì§¸ ìƒíƒœëŠ” 'ì´ˆê¸° ìƒíƒœ'ë¡œ ì„¤ì •\n",
        "    current_state = initial_state_json\n",
        "\n",
        "    # ìƒì„±ëœ Planì˜ ëª¨ë“  ë‹¨ê³„ë¥¼ ìˆœíšŒ\n",
        "    for i, step in enumerate(generated_plan_obj.task_plan):\n",
        "\n",
        "        step_description = step.description\n",
        "        print(f\"\\n--- 2ë‹¨ê³„ (Step {i+1}): '{step_description}'ìœ¼ë¡œ Action ìƒì„± ---\")\n",
        "\n",
        "        # â­ï¸ Task B í”„ë¡¬í”„íŠ¸ (í˜„ì¬ stateë¥¼ ì‚¬ìš©)\n",
        "        action_messages = [\n",
        "            {\"role\": \"system\",\n",
        "    \"content\": \"\"\"You are a specialized AI agent for Dongguk Univ nDRIMS. Your task is to analyze the user's input structure and perform one of two specific tasks:\n",
        "    1.  **Plan Generation:**\n",
        "      If the user 'prompt' contains **only** a `user_request` (e.g., \"ë¶ˆêµë™ì•„ë¦¬ ê°€ì…\"), this is a request for a plan.\n",
        "      You MUST respond with the complete `{\"task_plans\": [...]}` JSON for the entire task.\n",
        "\n",
        "    2.  **Action Generation:**\n",
        "      If the user 'prompt' contains a `task_plan` (current step) AND `observations` (current state), this is a request for an action.\n",
        "      You MUST respond with the single `{\"action\": {...}}` JSON for that specific step.\"\"\"},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"\n",
        "            {{'user_request': 'ë¶ˆêµë™ì•„ë¦¬',\n",
        "              'task_plan': '{step_description}',\n",
        "              'step_id': f'{i+1}',\n",
        "              'observations': {json.dumps(current_state, ensure_ascii=False)}}}\n",
        "            \"\"\"},\n",
        "        ]\n",
        "        action_prompt_string = tokenizer.apply_chat_template(\n",
        "            action_messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        action_result_obj = None # Pydantic ê°ì²´ë¥¼ ë‹´ì„ ë³€ìˆ˜\n",
        "        try:\n",
        "            action_result_str_or_obj = action_generator(action_prompt_string, max_new_tokens=256)\n",
        "\n",
        "            # Pydantic ê°ì²´/ë¬¸ìì—´ ë°˜í™˜ ì—¬ë¶€ í™•ì¸\n",
        "            if isinstance(action_result_str_or_obj, str):\n",
        "                print(f\"âš ï¸ 2ë‹¨ê³„ ê²½ê³ : Outlinesê°€ ë¬¸ìì—´ ë°˜í™˜. ìˆ˜ë™ íŒŒì‹± ì‹œë„...\")\n",
        "                print(f\"   ë°˜í™˜ëœ ì›ë³¸(str): {action_result_str_or_obj}\")\n",
        "                json_data = json.loads(action_result_str_or_obj)\n",
        "                action_result_obj = ActionModel.model_validate(json_data)\n",
        "            else:\n",
        "                action_result_obj = action_result_str_or_obj # Pydantic ê°ì²´\n",
        "\n",
        "            print(\"\\n--- â­ï¸ Action JSON ì¶œë ¥ (Step {i+1}) â­ï¸ ---\")\n",
        "            print(action_result_obj.model_dump_json(indent=2))\n",
        "\n",
        "            # === 3ë‹¨ê³„: â­ï¸ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì‹œë®¬ë ˆì´ì…˜) â­ï¸ ===\n",
        "            if action_result_obj.status == \"FINISH\":\n",
        "                print(\"\\n[INFO] 'FINISH' ìƒíƒœì´ë¯€ë¡œ ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "                break\n",
        "            else:\n",
        "                # (ì‹œë®¬ë ˆì´ì…˜) 1ë‹¨ê³„ê°€ ëë‚¬ìœ¼ë‹ˆ, 2ë‹¨ê³„ë¥¼ ìœ„í•´ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
        "                print(\"[INFO] (ì‹œë®¬ë ˆì´ì…˜) Action ì‹¤í–‰... ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "                # â­ï¸â­ï¸â­ï¸\n",
        "                # 1ë‹¨ê³„(\"í•™ìƒì‹ ì²­(ê¸°íƒ€)\" í´ë¦­)ê°€ ì‹¤í–‰ë˜ì—ˆìœ¼ë¯€ë¡œ,\n",
        "                # ë‹¤ìŒ ë£¨í”„ì—ì„œ ì‚¬ìš©í•  ìƒíƒœë¥¼ 'state_after_step_1_json'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.\n",
        "                # â­ï¸â­ï¸â­ï¸\n",
        "                current_state = state_after_step_1_json\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ğŸš¨ 2ë‹¨ê³„ (Step {i+1}) ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "            break # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë£¨í”„ ì¤‘ë‹¨\n",
        "\n",
        "    print(\"\\n--- ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ ---\")"
      ],
      "metadata": {
        "id": "gsyZPQyrSACh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NKiXh9VPXim"
      },
      "outputs": [],
      "source": [
        "user_input = input(\"ìš”ì²­ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"You are a helpful AI assistant for Dongguk Univ nDRIMS. Your job is to understand the user's request, formulate a step-by-step plan, and then output the correct action based on the current system observations (state)+ step-by-step plan + the user's request\"},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "    {'user_request': '{user_input}',\n",
        "     'observations': \"sidebar\": [\n",
        "    {\n",
        "      \"label\": \"ê°œì¸ì •ë³´ìˆ˜ì§‘ë™ì˜\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì‹ ì²­í•¨\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì§„í–‰í•¨\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì™„ë£Œí•¨\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"í•™ìƒì‹ ì²­(ê¸°íƒ€)\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"í•™ì /í™•ì¸ì„œ\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ìˆ˜ê°•ì‹ ì²­\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ìˆ˜ì—…/ê°•ì˜í‰ê°€\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì„±ì \",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¥í•™\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ë“±ë¡\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"êµì§\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¡¸ì—…\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ê³µí•™êµìœ¡ì¸ì¦\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"í˜„ì¥ì‹¤ìŠµ\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì˜ˆë¹„êµ°\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"êµìœ¡ì„¼í„°\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¨ë¨¸ìŠ¤ì¿¨\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ë‚¨ì‚°í•™ì‚¬\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¶©ë¬´í•™ì‚¬\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ê³ ì–‘í•™ì‚¬\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ê³µì§€ì‚¬í•­\",\n",
        "      \"expanded\": true,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": [\n",
        "        {\n",
        "          \"label\": \"ê³µì§€ì‚¬í•­ì¡°íšŒ\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": true,\n",
        "          \"sub_items\": []\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}}]}\n",
        "    \"\"\"},\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
        "\n",
        "    input_ids = torch.tensor([ids]).to(\"cuda\")\n",
        "    input_length = input_ids.shape[1]\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=64,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    generated_tokens = output[0][input_length:]\n",
        "    extracted_response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- Action step1 ---\")\n",
        "print(extracted_response)\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"You are a helpful AI assistant for Dongguk Univ nDRIMS. Your job is to understand the user's request, formulate a step-by-step plan, and then output the correct action based on the current system observations (state).\"},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "    {'user_request': '{user_input}',\n",
        "     'observations':  \"sidebar\": [\n",
        "    {\n",
        "      \"label\": \"ê°œì¸ì •ë³´ìˆ˜ì§‘ë™ì˜\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì‹ ì²­í•¨\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì§„í–‰í•¨\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ã€í•™ìƒì‹ ì²­ã€‘ì™„ë£Œí•¨\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"í•™ìƒì‹ ì²­(ê¸°íƒ€)\",\n",
        "      \"expanded\": true,\n",
        "      \"checked\": true,\n",
        "      \"sub_items\": [\n",
        "        {\n",
        "          \"label\": \"ë´‰ì‚¬ì†Œê°ë¬¸ë“±ë¡\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"ì‹ í–‰í™œë™ì‹ ì²­\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"ì‹ í–‰í™œë™ë§ˆì¼ë¦¬ì§€ì ìˆ˜í™•ì¸\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"í•™ìƒì‹ í–‰í™œë™ê¸°ì¤€í‘œì•ˆë‚´\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"êµì§ì´ìˆ˜ì‹ ì²­/ì·¨ì†Œì‹ ì²­\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"êµìœ¡ë´‰ì‚¬í™œë™ì‹œê°„ë“±ë¡\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"ì„±ì¸ì§€êµìœ¡ì‹ ì²­\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"ì‘ê¸‰ì²˜ì¹˜ë°ì‹¬íì†Œìƒìˆ  ì‹¤ìŠµì‹ ì²­\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"êµì§ì¸ì ì„±ê²€ì‚¬ ì‹ ì²­\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"êµì§ê¸°íƒ€ ì„œë¹„ìŠ¤ì‹ ì²­\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"DIYì„¤ë¬¸ì¡°ì‚¬\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"êµë¥˜ìƒí™©ë°íŒŒê²¬ì •ë³´ë“±ë¡\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"êµ­ì™¸êµë¥˜ì¸ì •ì„±ì ë“±ë¡\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"í•´ì™¸í•™ìˆ íƒë°©ì¼ì •ê´€ë¦¬\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"ê·¼ë¡œì¥í•™ê·¼ë¡œì‹œê°„ë“±ë¡\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"ë¶ˆêµë™ì•„ë¦¬ê°€ì…\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"í•™ìƒì¦ê°œì¸ì •ë³´ë™ì˜\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"DEISí•™ë¶€ëª¨ì„œë¹„ìŠ¤ì œê³µë™ì˜\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        },\n",
        "        {\n",
        "          \"label\": \"íˆ¬í‘œí•˜ê¸°\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"í•™ì /í™•ì¸ì„œ\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ìˆ˜ê°•ì‹ ì²­\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ìˆ˜ì—…/ê°•ì˜í‰ê°€\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì„±ì \",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¥í•™\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ë“±ë¡\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"êµì§\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¡¸ì—…\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ê³µí•™êµìœ¡ì¸ì¦\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"í˜„ì¥ì‹¤ìŠµ\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì˜ˆë¹„êµ°\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"êµìœ¡ì„¼í„°\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¨ë¨¸ìŠ¤ì¿¨\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ë‚¨ì‚°í•™ì‚¬\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ì¶©ë¬´í•™ì‚¬\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ê³ ì–‘í•™ì‚¬\",\n",
        "      \"expanded\": false,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": []\n",
        "    },\n",
        "    {\n",
        "      \"label\": \"ê³µì§€ì‚¬í•­\",\n",
        "      \"expanded\": true,\n",
        "      \"checked\": false,\n",
        "      \"sub_items\": [\n",
        "        {\n",
        "          \"label\": \"ê³µì§€ì‚¬í•­ì¡°íšŒ\",\n",
        "          \"expanded\": false,\n",
        "          \"checked\": false,\n",
        "          \"sub_items\": []\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}}]}}}\n",
        "    \"\"\"},\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
        "\n",
        "    input_ids = torch.tensor([ids]).to(\"cuda\")\n",
        "    input_length = input_ids.shape[1]\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=64,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    generated_tokens = output[0][input_length:]\n",
        "    extracted_response2 = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- Action step2 ---\")\n",
        "print(extracted_response2)\n",
        "# extracted_response ë³€ìˆ˜ì— JSON í˜•íƒœì˜ ë¬¸ìì—´ì´ ì €ì¥ë¨\n",
        "# {'function': 'click', 'control_label': 'ì·¨ë“í•™ì í™•ì¸ì„œì¡°íšŒ', ... 'status': 'FINISH'}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMjobb6ZPv5jZUsXApSUnxV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}